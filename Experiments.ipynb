{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IOHexperimenter import IOH_function, IOH_logger, IOHexperimenter\n",
    "import numpy as np\n",
    "import sys\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 10000\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# hyperparameters\n",
    "npop = 50 # population size \n",
    "beta = np.pi/36 # 5 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_1(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Global Sigma Mutation\n",
    "        - Intermediate Recombination\n",
    "        - (mu + lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    \n",
    "    tao_0 = 1/np.sqrt(n) # learning rate (tao 0)\n",
    "    lamda = npop * 7 # offspring population size\n",
    "    \n",
    "    fopt = -sys.maxsize-1\n",
    "    \n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0],high=problem.upperbound[0], size=(npop, n))\n",
    "    \n",
    "    sigmas = [(problem.upperbound[0]-problem.lowerbound[0])/6]*lamda # Global Sigma initialization with feasible range\n",
    "    \n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1*np.apply_along_axis(problem, 1, P)\n",
    "    \n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "    \n",
    "    \n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem. \n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n: \n",
    "        OP = [] # offspring population\n",
    "        \n",
    "        ### Intermediate Recombination\n",
    "        for i in range(lamda):\n",
    "            parent1 = P[np.random.choice(npop)]\n",
    "            parent2 = P[np.random.choice(npop)]\n",
    "            offspring = np.mean((parent1, parent2), axis=0)\n",
    "            OP.append(offspring)\n",
    "        \n",
    "        OP = np.array(OP) # Offspring population\n",
    "        OP_prime = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Mutation\n",
    "        for j, x in enumerate(OP):\n",
    "            sigmas[j] = sigmas[j] * np.exp(np.random.normal(0, tao_0))\n",
    "            x_prime = x + np.random.normal(0, sigmas[j], size=n) \n",
    "            OP_prime.append(x_prime)\n",
    "            \n",
    "        OP_prime = np.array(OP_prime)\n",
    "        \n",
    "        ### Evaluation of (OP_prime + P)\n",
    "        total_pop = np.append(P,OP_prime, axis=0)\n",
    "        fitness = -1*np.apply_along_axis(problem, 1, total_pop)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = total_pop[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "            \n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = total_pop[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_2(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Individual Sigma Mutation\n",
    "        - Intermediate Recombination\n",
    "        - (mu + lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    \n",
    "    tao_prime = 1/np.sqrt(2*n) # global learning rate\n",
    "    tao_0 = 1/np.sqrt(2*np.sqrt(n)) # local learning rate\n",
    "    \n",
    "    lamda = npop * 7 # offspring population size\n",
    "    \n",
    "    fopt = -sys.maxsize-1\n",
    "    \n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0],high=problem.upperbound[0], size=(npop, n))\n",
    "    \n",
    "    sigmas = [[(problem.upperbound[0]-problem.lowerbound[0])/6]*n]*lamda # Individual sigma initialization with feasible range\n",
    "    \n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1*np.apply_along_axis(problem, 1, P)\n",
    "    \n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "    \n",
    "    \n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem. \n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n: \n",
    "        OP = [] # offspring population\n",
    "        \n",
    "        ### Intermediate Recombination\n",
    "        for i in range(lamda):\n",
    "            parent1 = P[np.random.choice(npop)]\n",
    "            parent2 = P[np.random.choice(npop)]\n",
    "            offspring = np.mean((parent1, parent2), axis=0)\n",
    "            OP.append(offspring)\n",
    "        \n",
    "        OP = np.array(OP) # Offspring population\n",
    "        OP_prime = []\n",
    "        \n",
    "        N_tao_prime = np.random.normal(0, tao_prime)\n",
    "        \n",
    "        ### Mutation\n",
    "        for x, sigma in zip(OP, sigmas):\n",
    "            for x_i, sigma_i in zip(x, sigma):\n",
    "                sigma_i = sigma_i * np.exp(N_tao_prime + np.random.normal(0, tao_0))\n",
    "                x_i = x_i + np.random.normal(0, sigma_i)\n",
    "            OP_prime.append(x)\n",
    "            \n",
    "        OP_prime = np.array(OP_prime)\n",
    "        \n",
    "        ### Evaluation of (OP_prime + P)\n",
    "        total_pop = np.append(P,OP_prime, axis=0)\n",
    "        fitness = -1*np.apply_along_axis(problem, 1, total_pop)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = total_pop[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "            \n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = total_pop[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_3(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Correlated Mutation\n",
    "        - Intermediate Recombination\n",
    "        - (mu, lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    n_q = int(n * (n - 1) / 2)\n",
    "\n",
    "    tao_prime = 1 / np.sqrt(2 * n)  # global learning rate\n",
    "    tao_0 = 1 / np.sqrt(2 * np.sqrt(n))  # local learning rate\n",
    "\n",
    "    lamda = npop * 7  # offspring population size\n",
    "\n",
    "    fopt = -sys.maxsize - 1\n",
    "\n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0], high=problem.upperbound[0], size=(npop, n))\n",
    "\n",
    "    alphas = np.random.uniform(low=-np.pi, high=np.pi, size=(lamda, n_q))  # rotation angles initialized\n",
    "\n",
    "    OOB_correction = lambda x: x - 2*np.pi*(x / np.abs(x)) if (np.abs(x) > np.pi).any() else x  # out of boundary correction function\n",
    "\n",
    "    sigmas = [[(problem.upperbound[0] - problem.lowerbound[0]) / 6] * n] * lamda  # Individual sigma initialization with feasible range\n",
    "\n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1 * np.apply_along_axis(problem, 1, P)\n",
    "\n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "\n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem.\n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n:\n",
    "        OP = []  # offspring population\n",
    "        n_q = int(n * (n - 1) / 2)\n",
    "        ### Intermediate Recombination\n",
    "        for i in range(lamda):\n",
    "            parent1 = P[np.random.choice(npop)]\n",
    "            parent2 = P[np.random.choice(npop)]\n",
    "            offspring = np.mean((parent1, parent2), axis=0)\n",
    "            OP.append(offspring)\n",
    "\n",
    "        OP = np.array(OP)  # Offspring population\n",
    "\n",
    "        N_tao_prime = np.random.normal(0, tao_prime)\n",
    "\n",
    "        ### Mutation\n",
    "        # Step size update\n",
    "        sigmas_prime = [[sigma_i * np.exp(N_tao_prime + np.random.normal(0, tao_0)) for sigma_i in sigma] for sigma in sigmas]\n",
    "\n",
    "        # Rotation angles update\n",
    "        alphas_prime =  alphas + np.random.normal(0, beta, size=(lamda, n_q))\n",
    "        for alpha_prime in alphas_prime:\n",
    "            alpha_prime = np.apply_along_axis(OOB_correction, 0, alpha_prime)\n",
    "        \n",
    "        # Correlation of steps sizes\n",
    "        OP_prime = []\n",
    "        for j, x, sigma_prime in zip(range(lamda), OP, sigmas_prime):\n",
    "            s = np.zeros(n)\n",
    "            n_q = int(n * (n - 1) / 2)\n",
    "            for n_i in range(n):\n",
    "                s[n_i] = sigma_prime[n_i] * np.random.normal(0, 1)  # uncorrelated mutation vector initialization\n",
    "\n",
    "            for k in range(n-1):\n",
    "                n_i = n-1-k\n",
    "                n_ii = n-1\n",
    "                for i in range(k):\n",
    "                    d1, d2 = s[n_i], s[n_ii]\n",
    "                    s[n_ii] = d1 * np.sin(alphas_prime[j,n_q-1]) + d2 * np.cos(alphas_prime[j,n_q-1])\n",
    "                    s[n_i] = d1 * np.cos(alphas_prime[j,n_q-1]) - d2 * np.sin(alphas_prime[j,n_q-1])\n",
    "                    n_ii = n_ii - 1\n",
    "                    n_q = n_q - 1\n",
    "                \n",
    "            x_prime = x + s\n",
    "            OP_prime.append(x_prime)\n",
    "        OP_prime = np.array(OP_prime)\n",
    "\n",
    "        ### Evaluation of OP_prime\n",
    "        fitness = -1 * np.apply_along_axis(problem, 1, OP_prime)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = OP_prime[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "\n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = OP_prime[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_4(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Correlated Mutation\n",
    "        - Intermediate Recombination\n",
    "        - (mu + lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    n_q = int(n * (n - 1) / 2)\n",
    "\n",
    "    tao_prime = 1 / np.sqrt(2 * n)  # global learning rate\n",
    "    tao_0 = 1 / np.sqrt(2 * np.sqrt(n))  # local learning rate\n",
    "\n",
    "    lamda = npop * 7  # offspring population size\n",
    "\n",
    "    fopt = -sys.maxsize - 1\n",
    "\n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0], high=problem.upperbound[0], size=(npop, n))\n",
    "\n",
    "    alphas = np.random.uniform(low=-np.pi, high=np.pi, size=(lamda, n_q))  # rotation angles initialized\n",
    "\n",
    "    OOB_correction = lambda x: x - 2*np.pi*(x / np.abs(x)) if (np.abs(x) > np.pi).any() else x  # out of boundary correction function\n",
    "\n",
    "    sigmas = [[(problem.upperbound[0] - problem.lowerbound[0]) / 6] * n] * lamda  # Individual sigma initialization with feasible range\n",
    "\n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1 * np.apply_along_axis(problem, 1, P)\n",
    "\n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "\n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem.\n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n:\n",
    "        OP = []  # offspring population\n",
    "        n_q = int(n * (n - 1) / 2)\n",
    "        ### Intermediate Recombination\n",
    "        for i in range(lamda):\n",
    "            parent1 = P[np.random.choice(npop)]\n",
    "            parent2 = P[np.random.choice(npop)]\n",
    "            offspring = np.mean((parent1, parent2), axis=0)\n",
    "            OP.append(offspring)\n",
    "\n",
    "        OP = np.array(OP)  # Offspring population\n",
    "\n",
    "        N_tao_prime = np.random.normal(0, tao_prime)\n",
    "\n",
    "        ### Mutation\n",
    "        # Step size update\n",
    "        sigmas_prime = [[sigma_i * np.exp(N_tao_prime + np.random.normal(0, tao_0)) for sigma_i in sigma] for sigma in sigmas]\n",
    "\n",
    "        # Rotation angles update\n",
    "        alphas_prime =  alphas + np.random.normal(0, beta, size=(lamda, n_q))\n",
    "        for alpha_prime in alphas_prime:\n",
    "            alpha_prime = np.apply_along_axis(OOB_correction, 0, alpha_prime)\n",
    "        \n",
    "        # Correlation of steps sizes\n",
    "        OP_prime = []\n",
    "        for j, x, sigma_prime in zip(range(lamda), OP, sigmas_prime):\n",
    "            s = np.zeros(n)\n",
    "            n_q = int(n * (n - 1) / 2)\n",
    "            for n_i in range(n):\n",
    "                s[n_i] = sigma_prime[n_i] * np.random.normal(0, 1)  # uncorrelated mutation vector initialization\n",
    "\n",
    "            for k in range(n-1):\n",
    "                n_i = n-1-k\n",
    "                n_ii = n-1\n",
    "                for i in range(k):\n",
    "                    d1, d2 = s[n_i], s[n_ii]\n",
    "                    s[n_ii] = d1 * np.sin(alphas_prime[j,n_q-1]) + d2 * np.cos(alphas_prime[j,n_q-1])\n",
    "                    s[n_i] = d1 * np.cos(alphas_prime[j,n_q-1]) - d2 * np.sin(alphas_prime[j,n_q-1])\n",
    "                    n_ii = n_ii - 1\n",
    "                    n_q = n_q - 1\n",
    "                \n",
    "            x_prime = x + s\n",
    "            OP_prime.append(x_prime)\n",
    "        OP_prime = np.array(OP_prime)\n",
    "\n",
    "        ### Evaluation of (OP_prime + P)\n",
    "        total_pop = np.append(P,OP_prime, axis=0)\n",
    "        fitness = -1*np.apply_along_axis(problem, 1, total_pop)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = total_pop[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "            \n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = total_pop[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_5(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Global Sigma Mutation\n",
    "        - Intermediate Recombination\n",
    "        - (mu, lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    \n",
    "    tao_0 = 1/np.sqrt(n) # learning rate (tao 0)\n",
    "    lamda = npop * 7 # offspring population size\n",
    "    \n",
    "    fopt = -sys.maxsize-1\n",
    "    \n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0],high=problem.upperbound[0], size=(npop, n))\n",
    "    \n",
    "    sigmas = [(problem.upperbound[0]-problem.lowerbound[0])/6]*lamda # Global Sigma initialization with feasible range\n",
    "    \n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1*np.apply_along_axis(problem, 1, P)\n",
    "    \n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "    \n",
    "    \n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem. \n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n: \n",
    "        OP = [] # offspring population\n",
    "        \n",
    "        ### Intermediate Recombination\n",
    "        for i in range(lamda):\n",
    "            parent1 = P[np.random.choice(npop)]\n",
    "            parent2 = P[np.random.choice(npop)]\n",
    "            offspring = np.mean((parent1, parent2), axis=0)\n",
    "            OP.append(offspring)\n",
    "        \n",
    "        OP = np.array(OP) # Offspring population\n",
    "        OP_prime = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Mutation\n",
    "        for j, x in enumerate(OP):\n",
    "            sigmas[j] = sigmas[j] * np.exp(np.random.normal(0, tao_0))\n",
    "            x_prime = x + np.random.normal(0, sigmas[j], size=n) \n",
    "            OP_prime.append(x_prime)\n",
    "            \n",
    "        OP_prime = np.array(OP_prime)\n",
    "        \n",
    "        ### Evaluation of OP_prime\n",
    "        fitness = -1 * np.apply_along_axis(problem, 1, OP_prime)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = OP_prime[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "\n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = OP_prime[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_6(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Individual Sigma Mutation\n",
    "        - Intermediate Recombination\n",
    "        - (mu, lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    \n",
    "    tao_prime = 1/np.sqrt(2*n) # global learning rate\n",
    "    tao_0 = 1/np.sqrt(2*np.sqrt(n)) # local learning rate\n",
    "    \n",
    "    lamda = npop * 7 # offspring population size\n",
    "    \n",
    "    fopt = -sys.maxsize-1\n",
    "    \n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0],high=problem.upperbound[0], size=(npop, n))\n",
    "    \n",
    "    sigmas = [[(problem.upperbound[0]-problem.lowerbound[0])/6]*n]*lamda # Individual sigma initialization with feasible range\n",
    "    \n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1*np.apply_along_axis(problem, 1, P)\n",
    "    \n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "    \n",
    "    \n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem. \n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n: \n",
    "        OP = [] # offspring population\n",
    "        \n",
    "        ### Intermediate Recombination\n",
    "        for i in range(lamda):\n",
    "            parent1 = P[np.random.choice(npop)]\n",
    "            parent2 = P[np.random.choice(npop)]\n",
    "            offspring = np.mean((parent1, parent2), axis=0)\n",
    "            OP.append(offspring)\n",
    "        \n",
    "        OP = np.array(OP) # Offspring population\n",
    "        OP_prime = []\n",
    "        \n",
    "        N_tao_prime = np.random.normal(0, tao_prime)\n",
    "        \n",
    "        ### Mutation\n",
    "        for x, sigma in zip(OP, sigmas):\n",
    "            for x_i, sigma_i in zip(x, sigma):\n",
    "                sigma_i = sigma_i * np.exp(N_tao_prime + np.random.normal(0, tao_0))\n",
    "                x_i = x_i + np.random.normal(0, sigma_i)\n",
    "            OP_prime.append(x)\n",
    "            \n",
    "        OP_prime = np.array(OP_prime)\n",
    "        \n",
    "        ### Evaluation of OP_prime\n",
    "        fitness = -1 * np.apply_along_axis(problem, 1, OP_prime)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = OP_prime[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "\n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = OP_prime[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_7(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Global Sigma Mutation\n",
    "        - Global Intermediate Recombination\n",
    "        - (mu + lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    \n",
    "    tao_0 = 1/np.sqrt(n) # learning rate (tao 0)\n",
    "    lamda = npop * 7 # offspring population size\n",
    "    \n",
    "    fopt = -sys.maxsize-1\n",
    "    \n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0],high=problem.upperbound[0], size=(npop, n))\n",
    "    \n",
    "    sigmas = [(problem.upperbound[0]-problem.lowerbound[0])/6]*lamda # Global Sigma initialization with feasible range\n",
    "    \n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1*np.apply_along_axis(problem, 1, P)\n",
    "    \n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "    \n",
    "    \n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem. \n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n: \n",
    "        OP_prime = [] # offspring population after mutation\n",
    "        \n",
    "        ### Global Intermediate Recombination\n",
    "        for j in range(lamda):\n",
    "            offspring = np.mean(P, axis=0)\n",
    "            ### Mutation\n",
    "            sigmas[j] = sigmas[j] * np.exp(np.random.normal(0, tao_0))\n",
    "            x_prime = offspring + np.random.normal(0, sigmas[j], size=n) \n",
    "            OP_prime.append(x_prime)\n",
    "            \n",
    "        OP_prime = np.array(OP_prime)\n",
    "        \n",
    "        ### Evaluation of (OP_prime + P)\n",
    "        total_pop = np.append(P,OP_prime, axis=0)\n",
    "        fitness = -1*np.apply_along_axis(problem, 1, total_pop)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = total_pop[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "            \n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = total_pop[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_8(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Global Sigma Mutation\n",
    "        - Global Intermediate Recombination\n",
    "        - (mu, lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    \n",
    "    tao_0 = 1/np.sqrt(n) # learning rate (tao 0)\n",
    "    lamda = npop * 7 # offspring population size\n",
    "    \n",
    "    fopt = -sys.maxsize-1\n",
    "    \n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0],high=problem.upperbound[0], size=(npop, n))\n",
    "    \n",
    "    sigmas = [(problem.upperbound[0]-problem.lowerbound[0])/6]*lamda # Global Sigma initialization with feasible range\n",
    "    \n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1*np.apply_along_axis(problem, 1, P)\n",
    "    \n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "    \n",
    "    \n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem. \n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n: \n",
    "        OP_prime = [] # offspring population after mutation\n",
    "        \n",
    "        ### Global Intermediate Recombination\n",
    "        for j in range(lamda):\n",
    "            offspring = np.mean(P, axis=0)\n",
    "            ### Mutation\n",
    "            sigmas[j] = sigmas[j] * np.exp(np.random.normal(0, tao_0))\n",
    "            x_prime = offspring + np.random.normal(0, sigmas[j], size=n) \n",
    "            OP_prime.append(x_prime)\n",
    "            \n",
    "        OP_prime = np.array(OP_prime)\n",
    "        \n",
    "        ### Evaluation of OP_prime\n",
    "        fitness = -1 * np.apply_along_axis(problem, 1, OP_prime)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = OP_prime[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "\n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = OP_prime[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_9(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Individual Sigma Mutation\n",
    "        - Global Intermediate Recombination\n",
    "        - (mu + lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    \n",
    "    tao_prime = 1/np.sqrt(2*n) # global learning rate\n",
    "    tao_0 = 1/np.sqrt(2*np.sqrt(n)) # local learning rate\n",
    "    \n",
    "    lamda = npop * 7 # offspring population size\n",
    "    \n",
    "    fopt = -sys.maxsize-1\n",
    "    \n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0],high=problem.upperbound[0], size=(npop, n))\n",
    "    \n",
    "    sigmas = [[(problem.upperbound[0]-problem.lowerbound[0])/6]*n]*lamda # Individual sigma initialization with feasible range\n",
    "    \n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1*np.apply_along_axis(problem, 1, P)\n",
    "    \n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "    \n",
    "    \n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem. \n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n: \n",
    "        OP_prime = [] # offspring population after mutation\n",
    "        N_tao_prime = np.random.normal(0, tao_prime)\n",
    "        \n",
    "        ### Global Intermediate Recombination\n",
    "        for j in range(lamda):\n",
    "            offspring = np.mean(P, axis=0)\n",
    "            ### Mutation\n",
    "            for x_i, sigma_i in zip(offspring, sigmas[j]):\n",
    "                sigma_i = sigma_i * np.exp(N_tao_prime + np.random.normal(0, tao_0))\n",
    "                x_i = x_i + np.random.normal(0, sigma_i)\n",
    "            OP_prime.append(offspring)\n",
    "            \n",
    "        OP_prime = np.array(OP_prime)\n",
    "        \n",
    "        ### Evaluation of (OP_prime + P)\n",
    "        total_pop = np.append(P,OP_prime, axis=0)\n",
    "        fitness = -1*np.apply_along_axis(problem, 1, total_pop)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = total_pop[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "            \n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = total_pop[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_10(problem):\n",
    "    \"\"\"\n",
    "    This function uses:\n",
    "        - Correlated Mutation\n",
    "        - Global Intermediate Recombination\n",
    "        - (mu + lambda) selection\n",
    "    \"\"\"\n",
    "    n = problem.number_of_variables\n",
    "    n_q = int(n * (n - 1) / 2)\n",
    "\n",
    "    tao_prime = 1 / np.sqrt(2 * n)  # global learning rate\n",
    "    tao_0 = 1 / np.sqrt(2 * np.sqrt(n))  # local learning rate\n",
    "\n",
    "    lamda = npop * 7  # offspring population size\n",
    "\n",
    "    fopt = -sys.maxsize - 1\n",
    "\n",
    "    # Initial Population samples uniformly distributed over the interval (boundaries)\n",
    "    P = np.random.uniform(low=problem.lowerbound[0], high=problem.upperbound[0], size=(npop, n))\n",
    "\n",
    "    alphas = np.random.uniform(low=-np.pi, high=np.pi, size=(lamda, n_q))  # rotation angles initialized\n",
    "\n",
    "    OOB_correction = lambda x: x - 2*np.pi*(x / np.abs(x)) if (np.abs(x) > np.pi).any() else x  # out of boundary correction function\n",
    "\n",
    "    sigmas = [[(problem.upperbound[0] - problem.lowerbound[0]) / 6] * n] * lamda  # Individual sigma initialization with feasible range\n",
    "\n",
    "    # Fitness evaluation of the population\n",
    "    fitness = -1 * np.apply_along_axis(problem, 1, P)\n",
    "\n",
    "    if np.max(fitness) >= fopt:\n",
    "        x_prime = P[np.argmax(fitness)]\n",
    "        fopt = np.max(fitness)\n",
    "\n",
    "    ## !! final_target_hit returns True if the optimum has been found.\n",
    "    ## !! evaluations returns the number of function evaluations has been done on the problem.\n",
    "    while not problem.final_target_hit and problem.evaluations < budget * n:\n",
    "        OP_prime = []  # offspring population\n",
    "        n_q = int(n * (n - 1) / 2)\n",
    "        N_tao_prime = np.random.normal(0, tao_prime)\n",
    "        \n",
    "        # Rotation angles update\n",
    "        alphas_prime =  alphas + np.random.normal(0, beta, size=(lamda, n_q))\n",
    "        for alpha_prime in alphas_prime:\n",
    "            alpha_prime = np.apply_along_axis(OOB_correction, 0, alpha_prime)\n",
    "        \n",
    "        ### Global Intermediate Recombination\n",
    "        for j in range(lamda):\n",
    "            offspring = np.mean(P, axis=0)\n",
    "            ### Mutation\n",
    "            # Step size update\n",
    "            sigmas[j] = [sigma_i * np.exp(N_tao_prime + np.random.normal(0, tao_0)) for sigma_i in sigmas[j]]\n",
    "            \n",
    "            # Correlation of steps sizes\n",
    "            s = np.zeros(n)\n",
    "            n_q = int(n * (n - 1) / 2)\n",
    "            for n_i in range(n):\n",
    "                s[n_i] = sigmas[j][n_i] * np.random.normal(0, 1)  # uncorrelated mutation vector initialization\n",
    "\n",
    "            for k in range(n-1):\n",
    "                n_i = n-1-k\n",
    "                n_ii = n-1\n",
    "                for i in range(k):\n",
    "                    d1, d2 = s[n_i], s[n_ii]\n",
    "                    s[n_ii] = d1 * np.sin(alphas_prime[j,n_q-1]) + d2 * np.cos(alphas_prime[j,n_q-1])\n",
    "                    s[n_i] = d1 * np.cos(alphas_prime[j,n_q-1]) - d2 * np.sin(alphas_prime[j,n_q-1])\n",
    "                    n_ii = n_ii - 1\n",
    "                    n_q = n_q - 1\n",
    "                \n",
    "            x_prime = offspring + s\n",
    "            OP_prime.append(x_prime)\n",
    "        OP_prime = np.array(OP_prime)\n",
    "\n",
    "        ### Evaluation of (OP_prime + P)\n",
    "        total_pop = np.append(P,OP_prime, axis=0)\n",
    "        fitness = -1*np.apply_along_axis(problem, 1, total_pop)\n",
    "        if np.max(fitness) >= fopt:\n",
    "            x_prime = total_pop[np.argmax(fitness)]\n",
    "            fopt = np.max(fitness)\n",
    "            \n",
    "        ### Selection\n",
    "        top_fits = heapq.nlargest(npop, fitness)\n",
    "        sorter = np.argsort(fitness)\n",
    "        indices = sorter[np.searchsorted(fitness, top_fits, sorter=sorter)]\n",
    "        P = total_pop[indices]\n",
    "\n",
    "    return x_prime, fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1 finished!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-a7afc7bab3b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIOH_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BBOB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mxopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mES_10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Problem {p_id} finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-076ac08f8bb6>\u001b[0m in \u001b[0;36mES_10\u001b[0;34m(problem)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_ii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_ii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_q\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_q\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_q\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_q\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mn_ii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_ii\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    ## Declarations of Ids, instances, and dimensions that the problems to be tested.\n",
    "    problem_id = range(1,5)\n",
    "    instance_id = range(1,2)\n",
    "    dimension = [2,5]\n",
    "\n",
    "    ## Declariation of IOHprofiler_csv_logger.\n",
    "    ## 'result' is the name of output folder.\n",
    "    ## 'studentname1_studentname2' represents algorithm name and algorithm info, which will be caption of the algorithm in IOHanalyzer.\n",
    "    logger = IOH_logger(\"./\", \"Debug-result-ES10\", \"ES_10\", \"ES_10\")\n",
    "\n",
    "    for p_id in problem_id :\n",
    "        for d in dimension :\n",
    "            for i_id in instance_id:\n",
    "                ## Getting the problem with corresponding id,dimension, and instance.\n",
    "                f = IOH_function(p_id, d, i_id, suite=\"BBOB\")\n",
    "                f.add_logger(logger)\n",
    "                xopt, fopt = ES_10(f)\n",
    "        print(f\"Problem {p_id} finished!\")\n",
    "    logger.clear_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = ES_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'range' object has no attribute 'number_of_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b884802c179d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-27ca07775614>\u001b[0m in \u001b[0;36mES_1\u001b[0;34m(problem)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtao_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# learning rate (tao 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'range' object has no attribute 'number_of_variables'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ea",
   "language": "python",
   "name": "ea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
